{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
      "PyTorch version: 2.2.2\n",
      "Transformers version: 4.44.2\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA Version: 12.1\n",
      "FlashAttention available: True\n",
      "Retrieved HuggingFace token(s) from .env file\n",
      "Using HuggingFace token: hf_M*****************************IASJ\n",
      "Using HuggingFace write token: hf_u*****************************Xipx\n"
     ]
    }
   ],
   "source": [
    "dotenv_path = \"../../../../../apis/.env\"\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from textwrap import fill\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./src\")\n",
    "import text_classification_functions as tcf\n",
    "import env_options\n",
    "import lmsys_dataset_handler as lmsys\n",
    "\n",
    "hf_token, hf_token_write = env_options.check_env(colab=False, use_dotenv=True, dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lmsys_dataset_handler' from 'c:\\\\Users\\\\david\\\\Documents\\\\git\\\\chatbot-arena-wrapper\\\\./src\\\\lmsys_dataset_handler.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEBUG ###\n",
    "import importlib\n",
    "importlib.reload(lmsys)\n",
    "### DEBUG ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-memory sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from train-00002-of-00006-1779b7cec9462180.parquet\n",
      "Retrieved 100 random conversations from lmsys/lmsys-chat-1m/train-00002-of-00006-1779b7cec9462180.parquet\n",
      "Excluded 8 prompts.\n",
      "Extracted 96 prompts from lmsys/lmsys-chat-1m. Prompt sample:\n",
      "\n",
      "Please act as a Chinese-English translation coach. You first randomly generate a sentence in Chinese about any topic, and use 'question:' as a prompt. You will then ask the user to enter the English translation of the sentence. When the user enters the English translation of the sentence, you will evaluate the user's answer and provide the correct English translation of the Chinese sentence.Finally, please repeat the above three steps.\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = 100 # Number of full conversations to extract from the dataset: use a high number if streaming (samples chosen at random only if storing locally)\n",
    "MIN_CHAR_LENGTH = 50\n",
    "MAX_CHAR_LENGTH = 500 # Maximum character length of the prompts to be labeled\n",
    "exclusions = 'prompts/exclusions.txt'\n",
    "# exclusions = None\n",
    "\n",
    "lmsys_chat_1m = lmsys.LMSYSChat1MHandler(hf_token, streaming=False, verbose=False)\n",
    "df_sample = lmsys_chat_1m.parquet_sampling(n_samples=N_SAMPLES)\n",
    "# df_sample = lmsys_chat_1m.extract_df_sample(N_SAMPLES) # Slower\n",
    "df_prompts = lmsys_chat_1m.extract_prompts(filter_language=[], \n",
    "                                           min_char_length= MIN_CHAR_LENGTH,\n",
    "                                           max_char_length=MAX_CHAR_LENGTH,\n",
    "                                           exclusions=exclusions)\n",
    "# df_prompts.to_csv(\"csv/orig_sample.csv\", encoding='utf-8')\n",
    "\n",
    "prompt_sample = lmsys_chat_1m.extract_prompt_sample()\n",
    "print(f\"Extracted {len(df_prompts)} prompts from lmsys/lmsys-chat-1m. Prompt sample:\\n\")\n",
    "print(prompt_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print random conversation from sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation ID c92f1e63f567400ba37f87f5807d6e6d:\n",
      "\n",
      "ðŸ˜Ž hi\n",
      "\n",
      "ðŸ¤– Hello! How can I assist you today?\n",
      "\n",
      "ðŸ˜Ž are u gpt4?\n",
      "\n",
      "ðŸ¤– No, I am not GPT-4. I am a language model trained on a large dataset of text, but I am not the latest version of GPT. GPT-4, if it\n",
      "exists, is a future development and not yet publicly available.\n",
      "\n",
      "ðŸ˜Ž boring\n",
      "\n",
      "ðŸ¤– I'm sorry if my response was not interesting enough. Is there something specific you would like to know or discuss? I'll do my\n",
      "best to provide a more engaging response.\n",
      "\n",
      "ðŸ˜Ž u're not as good as gpt\n",
      "\n",
      "ðŸ¤– As a language model, I strive to provide the most accurate and informative responses to your questions to the best of my\n",
      "abilities. However, I acknowledge that there are limitations to my abilities and I may not always be able to provide the exact\n",
      "information you are looking for. If you have any specific questions, I will do my best to provide a helpful response.\n",
      "\n",
      "ðŸ˜Ž just downvoted u\n",
      "\n",
      "ðŸ¤– I'm sorry that my response did not meet your expectations. As an AI language model, I don't have feelings or emotions, so I can't\n",
      "be \"hurt\" by a downvote or any other form of feedback. However, I'm here to help you with any questions or information you need,\n",
      "so please let me know if there's anything specific I can assist you with.\n",
      "\n",
      "ðŸ˜Ž why did u assume i wanted u to feel hurt?\n",
      "\n",
      "ðŸ¤– I apologize if my response suggested that I assumed you wanted me to feel hurt. As an AI language model, I don't have the ability\n",
      "to assume or infer emotions, and I don't have the capacity to feel emotions. My previous response was simply an acknowledgement\n",
      "that as an AI language model, I don't have feelings or emotions and cannot be affected by feedback such as downvotes. I'm here to\n",
      "assist you with any questions or information you need, and I hope my responses are helpful to you.\n",
      "\n",
      "ðŸ˜Ž \"I don't have feelings or emotions, so I can't be \"hurt\" by a downvote or any other form of feedback.\" Seems passive aggressive\n",
      "for me...\n",
      "\n",
      "ðŸ¤– I apologize if my response came across as passive-aggressive. It was not intended that way. As an AI language model, I don't have\n",
      "feelings or emotions, and I don't have the ability to be hurt or affected by feedback such as downvotes. My response was simply an\n",
      "acknowledgement of that fact and an offer to assist you with any questions or information you need. Please let me know if there's\n",
      "anything specific I can help you with.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing an example of a multi-turn conversation\n",
    "df_sample_with_turns = lmsys_chat_1m.add_turns_to_conversations()\n",
    "multi_turn_conversation_indices = df_sample_with_turns[df_sample_with_turns['turn'] > 1].index\n",
    "random_conversation_index = random.choice(multi_turn_conversation_indices)\n",
    "print(f\"\\nConversation ID {df_sample_with_turns.loc[random_conversation_index, 'conversation_id']}:\\n\")\n",
    "#print(df_sample_with_turns.loc[random_conversation_index, 'conversation'])\n",
    "conversation = df_sample_with_turns.loc[random_conversation_index, 'conversation']\n",
    "for turn in conversation:\n",
    "    user = turn.get('role')\n",
    "    content = turn.get('content', '')\n",
    "    wrapped_content = fill(content, width=130)\n",
    "    role = 'ðŸ˜Ž' if user == 'user' else 'ðŸ¤–'\n",
    "    print(f\"{role} {wrapped_content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
